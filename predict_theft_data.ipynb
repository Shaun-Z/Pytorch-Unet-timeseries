{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from unet import UNet_1D, UNet_1D_N, UNet_1D_L, UNet_1D_NN\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(net,\n",
    "                data,\n",
    "                device,\n",
    "                out_threshold=0.5):\n",
    "    net.eval()\n",
    "    data = data.to(device=device, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(data.shape)\n",
    "        output = net(data).cpu()\n",
    "        # print(output.size())\n",
    "        if net.n_classes > 1:\n",
    "            mask = output.argmax(dim=1)\n",
    "        else:\n",
    "            mask = torch.sigmoid(output) > out_threshold\n",
    "        # print(mask.size())\n",
    "    return mask[0].long().squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 'SGCC'\n",
    "dir_checkpoint = Path(f'./checkpoints_pseudo_{id}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zx3_normalized = pd.read_csv(f'./data/{id}_data/data_prepared/zx2_normalized.csv') \n",
    "# normal3_normalized = pd.read_csv(f'./data/{id}_data/data_prepared/Normal3_normalized.csv')\n",
    "# # Rename the columns of df2 to match df1\n",
    "# normal3_normalized.columns = zx3_normalized.columns\n",
    "\n",
    "# zy3 = pd.read_csv(f'./data/{id}_data/data_prepared/zy2.csv') \n",
    "# normal3_normalized_label = pd.read_csv(f'./data/{id}_data/data_prepared/Normal3_normalized_label.csv')\n",
    "# normal3_normalized_label.columns = zy3.columns\n",
    "\n",
    "# combined_dfx = pd.concat([zx3_normalized, normal3_normalized], ignore_index=True)#\n",
    "# combined_dfy = pd.concat([zy3, normal3_normalized_label], ignore_index=True)#\n",
    "\n",
    "\n",
    "# combined_dfx.to_csv(f'./data/{id}_data/data_prepared/combined_dfx.csv', index=False)\n",
    "# combined_dfy.to_csv(f'./data/{id}_data/data_prepared/combined_dfy.csv', index=False)\n",
    "\n",
    "\n",
    "# # normal3_normalized_sudolabel = pd.read_csv(f'./Normal3_normalized_sudolabel.csv')\n",
    "# # normal3_normalized_sudolabel.columns = zy3.columns\n",
    "# # combined_dfy_sudo = pd.concat([zy3, normal3_normalized_sudolabel], ignore_index=True)#\n",
    "# # combined_dfy_sudo.to_csv('./combined_dfy_sudo.csv', index=False)\n",
    "\n",
    "# # normal3_normalized_sudolabel1 = pd.read_csv(f'./Normal3_normalized_sudolabel1.csv')\n",
    "# # normal3_normalized_sudolabel1.columns = zy3.columns\n",
    "# # combined_dfy_sudo1 = pd.concat([zy3, normal3_normalized_sudolabel1], ignore_index=True)#\n",
    "# # combined_dfy_sudo1.to_csv('./combined_dfy_sudo1.csv', index=False)\n",
    "\n",
    "# normal3_normalized_pseudolabel2 = pd.read_csv(f'./data/{id}_data/data_prepared/Normal3_normalized_pseudolabel.csv')\n",
    "# normal3_normalized_pseudolabel2.columns = zy3.columns\n",
    "# combined_dfy_pseudo2 = pd.concat([zy3, normal3_normalized_pseudolabel2], ignore_index=True)#\n",
    "# combined_dfy_pseudo2.to_csv(f'./data/{id}_data/data_prepared/combined_dfy_pseudo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_dfy_pseudo2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### testing 用的predict_t.py\n",
    "# id = ''\n",
    "##### 改变的地方是：\n",
    "dir_data = Path(f'./data/{id}_data/data_prepared/usable_theft.csv')\n",
    "\n",
    "data_val = pd.read_csv(dir_data)\n",
    "data_val = torch.tensor(data_val.values, dtype=torch.float32)\n",
    "mask_val = np.ones(data_val.shape)\n",
    "\n",
    "net = UNet_1D(n_channels=1, n_classes=2, bilinear=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net.to(device=device)\n",
    "# state_dict = torch.load('./checkpoints/checkpoint_epoch21.pth', map_location=device)\n",
    "# state_dict = torch.load('./checkpoints_Attack3_pure/checkpoint_epoch20.pth', map_location=device)##old attack 3\n",
    "# state_dict = torch.load('./checkpoints_Attack3_pure/checkpoint_epoch20_combined.pth', map_location=device)##old attack 3+new 3000 normal data\n",
    "# state_dict = torch.load('./checkpoints_Attack3_pure/checkpoint_epoch20_combine_sudo.pth', map_location=device)##old attack 3+new 3000 normal data+sudo label\n",
    "# state_dict = torch.load(f'./checkpoints_sudo_{id}/checkpoint_epoch20.pth', map_location=device)##old attack 3+new \n",
    "# 3000 normal data + pseudo label\n",
    "state_dict = torch.load(f'./checkpoints_pseudo_{id}/checkpoint_epoch20.pth', map_location=device)\n",
    "mask_values = state_dict.pop('mask_values', [0, 1])\n",
    "net.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val.shape, mask_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "pred_ts = []\n",
    "for i in range(len(data_val)):\n",
    "    item = data_val[i, :].unsqueeze(0).unsqueeze(0)\n",
    "    print(item.shape)\n",
    "    \n",
    "    mask = predict_data(net=net,\n",
    "                        data=item,\n",
    "                        out_threshold=0.5,\n",
    "                        device=device)\n",
    "\n",
    "    result.append(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(result)\n",
    "# result_df.to_csv('result.csv', index=False)\n",
    "zy = pd.DataFrame(np.array(mask_val).astype(int))\n",
    "# zy = pd.read_csv('./combined_dfy.csv')\n",
    "# zy = pd.read_csv(f'./data/{id}_data/data_prepared/combined_dfy_pseudo.csv')\n",
    "# zy = pd.read_csv('.\\Attack3_normalized_label.csv')\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "fig.suptitle(f'Attack 3')   \n",
    "\n",
    "# Plot the first heatmap\n",
    "sns.heatmap(result_df, ax=ax1)\n",
    "ax1.set_title('Heatmap of prediction')\n",
    "\n",
    "# Plot the second heatmap\n",
    "sns.heatmap(zy, ax=ax2)\n",
    "ax2.set_title('Heatmap of target')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'result{id}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(result_df.index, result_df.sum(axis=1))\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Sum of Predictions')\n",
    "plt.title('Scatter Plot of Sum of Predictions in result_df')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
